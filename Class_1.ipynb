{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class One: 211\n",
      "Class Two: 581\n",
      "Class Three: 20\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "NEW_FILE3 = 'D:/datasets/solar/new_data/new_data4.csv'\n",
    "data = np.loadtxt(NEW_FILE3,delimiter=',',skiprows=1,usecols=(0,1,3,4,5,6,7))#时间、月份、温度、湿度、辐射、漫反射、功率\n",
    "Ever_day = []\n",
    "for i in range(812):\n",
    "    tmp = data[i*21:i*21+21,:]\n",
    "    Ever_day.append(np.mean(tmp, 0))#按列求平均\n",
    "Ever_day = np.array(Ever_day)\n",
    "X = Ever_day[:,1:]\n",
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps = 28, min_samples = 5).fit(X)\n",
    "labels = db.labels_\n",
    "#print(set(labels))\n",
    "#print(sorted(labels))\n",
    "Ever_day = np.c_[Ever_day,labels]#在Every_day后增加一列\n",
    "class_one = []\n",
    "class_two = []\n",
    "class_three = []\n",
    "for row in Ever_day:\n",
    "    if row[-1] == -1:\n",
    "        class_one.append(row[0])\n",
    "    elif row[-1] == 0:\n",
    "        class_two.append(row[0])\n",
    "    else:\n",
    "        class_three.append(row[0])\n",
    "print('Class One:',len(class_one))\n",
    "print('Class Two:',len(class_two))\n",
    "print('Class Three:',len(class_three))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "制作数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class two : 571.0\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(NEW_FILE3,delimiter=',',skiprows=1)\n",
    "for i in range(len(data)):\n",
    "    if data[i][1] == 1:data[i][1] = 17.938\n",
    "    elif data[i][1] == 2:data[i][1] = 17.938\n",
    "    elif data[i][1] == 3:data[i][1] = 16.067\n",
    "    elif data[i][1] == 4:data[i][1] = 16.067\n",
    "    elif data[i][1] == 5:data[i][1] = 16.067\n",
    "    elif data[i][1] == 6:data[i][1] = 15.137\n",
    "    elif data[i][1] == 7:data[i][1] = 15.137\n",
    "    elif data[i][1] == 8:data[i][1] = 15.137\n",
    "    elif data[i][1] == 9:data[i][1] = 17.714\n",
    "    elif data[i][1] == 10:data[i][1] = 17.714\n",
    "    elif data[i][1] == 11:data[i][1] = 17.714\n",
    "    elif data[i][1] == 12:data[i][1] = 17.938\n",
    "    \n",
    "    if data[i][2] == 8.0:data[i][2] = 5.1096\n",
    "    elif data[i][2] == 8.5:data[i][2] = 8.9351\n",
    "    elif data[i][2] == 9.0:data[i][2] = 12.5525\n",
    "    elif data[i][2] == 9.5:data[i][2] = 15.4875\n",
    "    elif data[i][2] == 10.0:data[i][2] = 18.1367\n",
    "    elif data[i][2] == 10.5:data[i][2] = 20.2583\n",
    "    elif data[i][2] == 11.0:data[i][2] = 21.9231\n",
    "    elif data[i][2] == 11.5:data[i][2] = 23.1655\n",
    "    elif data[i][2] == 12.0:data[i][2] = 23.9481\n",
    "    elif data[i][2] == 12.5:data[i][2] = 24.0809\n",
    "    elif data[i][2] == 13.0:data[i][2] = 24.1543\n",
    "    elif data[i][2] == 13.5:data[i][2] = 23.6692\n",
    "    elif data[i][2] == 14.0:data[i][2] = 22.6423\n",
    "    elif data[i][2] == 14.5:data[i][2] = 21.1091\n",
    "    elif data[i][2] == 15.0:data[i][2] = 19.6702\n",
    "    elif data[i][2] == 15.5:data[i][2] = 17.5288\n",
    "    elif data[i][2] == 16.0:data[i][2] = 15.1380\n",
    "    elif data[i][2] == 16.5:data[i][2] = 12.1955\n",
    "    elif data[i][2] == 17.0:data[i][2] = 9.1260\n",
    "    elif data[i][2] == 17.5:data[i][2] = 5.4909\n",
    "    elif data[i][2] == 18.0:data[i][2] = 2.4679\n",
    "    \n",
    "    if data[i][-1] == 0:data[i][-1] = data[i][-3]*0.1\n",
    "\n",
    "skip_day = [1,16,31,38,69,81,126,182,197,280,301,320,330,413,484,498,579,622,636,651,683,739,749,860]\n",
    "skip_day = skip_day + class_one + class_three\n",
    "all_data = []\n",
    "for i in range(17052):\n",
    "    if data[i][0] in skip_day:\n",
    "        continue\n",
    "    elif i%21 == 0:\n",
    "        all_data.append(np.array([data[i][0],data[i][1],data[i][2],data[i][3],data[i][4],data[i][5],\n",
    "                                  data[i][6],data[i-21][7],data[i-21][7],data[i][7]]))\n",
    "    else:\n",
    "        all_data.append(np.array([data[i][0],data[i][1],data[i][2],data[i][3],data[i][4],data[i][5],\n",
    "                                  data[i][6],data[i-21][7],data[i-1][7],data[i][7]]))\n",
    "print('class two :',len(all_data)/21)\n",
    "#日期、月份、时刻、温度、湿度、辐射、漫反射、功率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.1538348503905211\n",
      "  0.15167657519874594\n",
      "  0.16329303840023213\n",
      "  0.15748205942543722\n",
      "  0.18120147004640944\n",
      "0.16149759869226915\n"
     ]
    }
   ],
   "source": [
    "f = []\n",
    "from sklearn import linear_model \n",
    "for i in [1,2,3,4,5]:\n",
    "    np.random.seed(i)\n",
    "    test_list = np.random.randint(0,int(len(all_data)/21),size = 114)#有重复\n",
    "    test_list = sorted(set(test_list))\n",
    "\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    train_date = []\n",
    "    test_date = []\n",
    "    for i in range(int(len(all_data)/21)):\n",
    "        for j in range(21):\n",
    "            a = all_data[i*21+j]\n",
    "            if i in test_list:\n",
    "                test_data.append(a[1:-1])\n",
    "                test_label.append(a[-1])\n",
    "                test_date.append(a[0])\n",
    "            else:\n",
    "                train_data.append(a[1:-1])\n",
    "                train_label.append(a[-1])\n",
    "                train_date.append(a[0])\n",
    "    LL = linear_model.LinearRegression()\n",
    "    LL.fit(train_data,train_label)\n",
    "    predict_power = LL.predict(test_data)\n",
    "    mape = [abs(predict_power[i]-test_label[i])/test_label[i] for i in range(len(test_label))]\n",
    "    f.append(np.mean(mape))\n",
    "    print(' ',np.mean(mape))\n",
    "print(np.mean(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LARs Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.1936204453446315\n",
      "  0.19702044488943976\n",
      "  0.20626788447737754\n",
      "  0.19533677799931767\n",
      "  0.20841290429997397\n",
      "0.2001316914021481\n"
     ]
    }
   ],
   "source": [
    "f = []\n",
    "for i in [1,2,3,4,5]:\n",
    "    np.random.seed(i)\n",
    "    test_list = np.random.randint(0,int(len(all_data)/21),size = 114)#有重复\n",
    "    test_list = sorted(set(test_list))\n",
    "\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    train_date = []\n",
    "    test_date = []\n",
    "    for i in range(int(len(all_data)/21)):\n",
    "        for j in range(21):\n",
    "            a = all_data[i*21+j]\n",
    "            if i in test_list:\n",
    "                test_data.append(a[1:-1])\n",
    "                test_label.append(a[-1])\n",
    "                test_date.append(a[0])\n",
    "            else:\n",
    "                train_data.append(a[1:-1])\n",
    "                train_label.append(a[-1])\n",
    "                train_date.append(a[0])\n",
    "    \n",
    "    LL = linear_model.LassoLars(alpha = 0.0001)\n",
    "    LL.fit(train_data,train_label)\n",
    "    predict_power = LL.predict(test_data)\n",
    "    mape = [abs(predict_power[i]-test_label[i])/test_label[i] for i in range(len(test_label))]\n",
    "    f.append(np.mean(mape))\n",
    "    print(' ',np.mean(mape))\n",
    "print(np.mean(f))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0860771327367422\n",
      "2 0.09084971469944327\n",
      "3 0.09927694762815822\n",
      "4 0.08646056312660459\n",
      "5 0.11407878871751628\n",
      "0.0953486293816929\n"
     ]
    }
   ],
   "source": [
    "f = []\n",
    "from sklearn import svm\n",
    "for x in [1,2,3,4,5]:\n",
    "    np.random.seed(x)\n",
    "    test_list = np.random.randint(0,int(len(all_data)/21),size = 114)#有重复\n",
    "    test_list = sorted(set(test_list))\n",
    "\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    train_date = []\n",
    "    test_date = []\n",
    "    for i in range(int(len(all_data)/21)):\n",
    "        for j in range(21):\n",
    "            a = all_data[i*21+j]\n",
    "            if i in test_list:\n",
    "                test_data.append(a[1:-1])\n",
    "                test_label.append(a[-1])\n",
    "                test_date.append(a[0])\n",
    "            else:\n",
    "                train_data.append(a[1:-1])\n",
    "                train_label.append(a[-1])\n",
    "                train_date.append(a[0])\n",
    "\n",
    "    LL = svm.SVR(kernel = 'rbf', C = 1000, gamma = 0.000064)\n",
    "    LL.fit(train_data,train_label)\n",
    "    predict_power = LL.predict(test_data)\n",
    "    mape = [abs(predict_power[i]-test_label[i])/test_label[i] for i in range(len(test_label))]\n",
    "    print(x,np.mean(mape))\n",
    "    f.append(np.mean(mape))\n",
    "print(np.mean(f))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutil Layers NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epoch,loss value:36533.8\n",
      "After 50 epoch,loss value:761.613\n",
      "After 100 epoch,loss value:493.97\n",
      "After 150 epoch,loss value:453.382\n",
      "After 200 epoch,loss value:439.98\n",
      "After 250 epoch,loss value:425.906\n",
      "After 300 epoch,loss value:417.704\n",
      "After 350 epoch,loss value:411.553\n",
      "After 400 epoch,loss value:405.501\n",
      "After 450 epoch,loss value:401.462\n",
      "After 500 epoch,loss value:395.973\n",
      "After 550 epoch,loss value:393.591\n",
      "After 600 epoch,loss value:393.516\n",
      "After 650 epoch,loss value:388.209\n",
      "After 700 epoch,loss value:386.146\n",
      "After 750 epoch,loss value:387.705\n",
      "After 800 epoch,loss value:384.064\n",
      "After 850 epoch,loss value:385.823\n",
      "After 900 epoch,loss value:382.285\n",
      "After 950 epoch,loss value:382.979\n",
      "After 1000 epoch,loss value:381.534\n",
      "Training Finished\n",
      "TEST MAPE: 0.09949819\n",
      "After 0 epoch,loss value:36610.3\n",
      "After 50 epoch,loss value:758.636\n",
      "After 100 epoch,loss value:501.984\n",
      "After 150 epoch,loss value:435.937\n",
      "After 200 epoch,loss value:409.408\n",
      "After 250 epoch,loss value:397.782\n",
      "After 300 epoch,loss value:388.191\n",
      "After 350 epoch,loss value:380.425\n",
      "After 400 epoch,loss value:377.912\n",
      "After 450 epoch,loss value:376.446\n",
      "After 500 epoch,loss value:374.264\n",
      "After 550 epoch,loss value:375.067\n",
      "After 600 epoch,loss value:372.01\n",
      "After 650 epoch,loss value:372.665\n",
      "After 700 epoch,loss value:372.542\n",
      "After 750 epoch,loss value:370.642\n",
      "After 800 epoch,loss value:367.008\n",
      "After 850 epoch,loss value:368.001\n",
      "After 900 epoch,loss value:369.366\n",
      "After 950 epoch,loss value:363.053\n",
      "After 1000 epoch,loss value:363.768\n",
      "Training Finished\n",
      "TEST MAPE: 0.10827068\n",
      "After 0 epoch,loss value:36896.9\n",
      "After 50 epoch,loss value:677.825\n",
      "After 100 epoch,loss value:508.948\n",
      "After 150 epoch,loss value:426.252\n",
      "After 200 epoch,loss value:402.376\n",
      "After 250 epoch,loss value:388.379\n",
      "After 300 epoch,loss value:379.456\n",
      "After 350 epoch,loss value:371.634\n",
      "After 400 epoch,loss value:368.323\n",
      "After 450 epoch,loss value:366.067\n",
      "After 500 epoch,loss value:364.215\n",
      "After 550 epoch,loss value:363.679\n",
      "After 600 epoch,loss value:366.299\n",
      "After 650 epoch,loss value:361.599\n",
      "After 700 epoch,loss value:361.886\n",
      "After 750 epoch,loss value:360.522\n",
      "After 800 epoch,loss value:362.923\n",
      "After 850 epoch,loss value:363.392\n",
      "After 900 epoch,loss value:376.651\n",
      "After 950 epoch,loss value:358.717\n",
      "After 1000 epoch,loss value:358.016\n",
      "Training Finished\n",
      "TEST MAPE: 0.11753868\n",
      "After 0 epoch,loss value:36615.2\n",
      "After 50 epoch,loss value:827.91\n",
      "After 100 epoch,loss value:496.302\n",
      "After 150 epoch,loss value:447.672\n",
      "After 200 epoch,loss value:419.292\n",
      "After 250 epoch,loss value:402.944\n",
      "After 300 epoch,loss value:393.177\n",
      "After 350 epoch,loss value:388.36\n",
      "After 400 epoch,loss value:385.587\n",
      "After 450 epoch,loss value:384.585\n",
      "After 500 epoch,loss value:382.202\n",
      "After 550 epoch,loss value:379.536\n",
      "After 600 epoch,loss value:378.381\n",
      "After 650 epoch,loss value:378.367\n",
      "After 700 epoch,loss value:377.616\n",
      "After 750 epoch,loss value:375.926\n",
      "After 800 epoch,loss value:376.36\n",
      "After 850 epoch,loss value:390.584\n",
      "After 900 epoch,loss value:373.142\n",
      "After 950 epoch,loss value:373.845\n",
      "After 1000 epoch,loss value:374.767\n",
      "Training Finished\n",
      "TEST MAPE: 0.09821363\n",
      "After 0 epoch,loss value:36675.2\n",
      "After 50 epoch,loss value:766.724\n",
      "After 100 epoch,loss value:485.858\n",
      "After 150 epoch,loss value:427.686\n",
      "After 200 epoch,loss value:406.219\n",
      "After 250 epoch,loss value:394.493\n",
      "After 300 epoch,loss value:388.764\n",
      "After 350 epoch,loss value:383.418\n",
      "After 400 epoch,loss value:380.53\n",
      "After 450 epoch,loss value:380.56\n",
      "After 500 epoch,loss value:373.461\n",
      "After 550 epoch,loss value:364.009\n",
      "After 600 epoch,loss value:364.685\n",
      "After 650 epoch,loss value:358.575\n",
      "After 700 epoch,loss value:361.935\n",
      "After 750 epoch,loss value:359.039\n",
      "After 800 epoch,loss value:354.92\n",
      "After 850 epoch,loss value:353.317\n",
      "After 900 epoch,loss value:354.484\n",
      "After 950 epoch,loss value:354.753\n",
      "After 1000 epoch,loss value:349.147\n",
      "Training Finished\n",
      "TEST MAPE: 0.11659065\n",
      "0.108022355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "INPUT_NODE = 8\n",
    "OUTPUT_NODE = 1\n",
    "LAYER1_NODE = 100\n",
    "LAYER2_NODE = 90\n",
    "LAYER3_NODE = 80\n",
    "LAYER4_NODE = 60\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.004\n",
    "REGULARIZATION_RATE = 0.0000001\n",
    "\n",
    "TRAINING_EPOCH = 1001\n",
    "tf.set_random_seed(1)\n",
    "def inference(input_data, regularizer):\n",
    "    W1 = tf.Variable(tf.truncated_normal([INPUT_NODE,LAYER1_NODE],stddev=0.05))\n",
    "    b1 = tf.Variable(tf.constant(0.1,shape=[LAYER1_NODE]))\n",
    "    layer1 = tf.matmul(input_data,W1)+b1\n",
    "    layer1 = tf.maximum(layer1, 0.1*layer1)\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',regularizer(W1))\n",
    "    \n",
    "    W2 = tf.Variable(tf.truncated_normal([LAYER1_NODE,LAYER2_NODE],stddev=0.05))\n",
    "    b2 = tf.Variable(tf.constant(0.1,shape=[LAYER2_NODE]))\n",
    "    layer2 = tf.matmul(layer1,W2)+b2\n",
    "    layer2 = tf.maximum(layer2, 0.1*layer2)\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',regularizer(W2))\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([LAYER2_NODE,LAYER3_NODE],stddev=0.05))\n",
    "    b3 = tf.Variable(tf.constant(0.1,shape=[LAYER3_NODE]))\n",
    "    layer3 = tf.matmul(layer2,W3)+b3\n",
    "    layer3 = tf.maximum(layer3, 0.1*layer3)\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',regularizer(W3))\n",
    "    \n",
    "    W4 = tf.Variable(tf.truncated_normal([LAYER3_NODE,LAYER4_NODE],stddev=0.05))\n",
    "    b4 = tf.Variable(tf.constant(0.1,shape=[LAYER4_NODE]))\n",
    "    layer4 = tf.matmul(layer3,W4)+b4\n",
    "    layer4 = tf.maximum(layer4, 0.1*layer4)\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',regularizer(W4))\n",
    "    \n",
    "    W_o = tf.Variable(tf.truncated_normal([LAYER4_NODE,OUTPUT_NODE],stddev=0.05))\n",
    "    b_o = tf.Variable(tf.constant(0.1,shape=[OUTPUT_NODE]))\n",
    "    pre = tf.nn.relu(tf.matmul(layer4,W_o)+b_o)\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',regularizer(W_o))\n",
    "        \n",
    "    return pre\n",
    "\n",
    "input_data = tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "input_label = tf.placeholder(tf.float32,[None,OUTPUT_NODE])\n",
    "regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "\n",
    "pre = inference(input_data,regularizer)\n",
    "test_pre = inference(input_data,None)\n",
    "cost = tf.reduce_mean(tf.square(input_label - pre)) + tf.add_n(tf.get_collection('losses'))\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cost)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    f = []\n",
    "    for i in [1,2,3,4,5]:\n",
    "        np.random.seed(i)\n",
    "        test_list = np.random.randint(0,int(len(all_data)/21),size = 114)#有重复\n",
    "        test_list = sorted(set(test_list))\n",
    "\n",
    "        train_data = []\n",
    "        train_label = []\n",
    "        test_data = []\n",
    "        test_label = []\n",
    "        train_date = []\n",
    "        test_date = []\n",
    "        for i in range(int(len(all_data)/21)):\n",
    "            for j in range(21):\n",
    "                a = all_data[i*21+j]\n",
    "                if i in test_list:\n",
    "                    test_data.append(a[1:-1])\n",
    "                    test_label.append(a[-1])\n",
    "                    test_date.append(a[0])\n",
    "                else:\n",
    "                    train_data.append(a[1:-1])\n",
    "                    train_label.append(a[-1])\n",
    "                    train_date.append(a[0])\n",
    "        BATCH_SIZE = len(train_data)\n",
    "        tf.global_variables_initializer().run()\n",
    "        start = 0\n",
    "        end = start+BATCH_SIZE\n",
    "        for i in range(int(TRAINING_EPOCH*len(train_data)/BATCH_SIZE)):\n",
    "            xs = train_data[start:end]\n",
    "            ys = train_label[start:end]\n",
    "            try:\n",
    "                ys = np.reshape(ys,[BATCH_SIZE,1])\n",
    "            except:\n",
    "                print(len(ys))\n",
    "                print(len(train_data))\n",
    "            _, loss = sess.run([train_step,cost],feed_dict={input_data:xs, input_label:ys})\n",
    "            if end == len(train_data):\n",
    "                start = 0\n",
    "                end = BATCH_SIZE\n",
    "            else:\n",
    "                start = end\n",
    "                end+=BATCH_SIZE\n",
    "            if i%(50*len(train_data)/BATCH_SIZE) == 0:\n",
    "                print('After %d epoch,loss value:%g'%(i/(len(train_data)/BATCH_SIZE),loss))\n",
    "        print('Training Finished')\n",
    "\n",
    "        predict_power = sess.run(pre,feed_dict={input_data:test_data})\n",
    "\n",
    "        mape = [abs(predict_power[i]-test_label[i])/test_label[i] for i in range(21*len(test_list))]\n",
    "        print('TEST MAPE:',np.mean(mape))\n",
    "        f.append(np.mean(mape))\n",
    "    print(np.mean(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
