{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 84\n"
     ]
    }
   ],
   "source": [
    "#读取数据\n",
    "import numpy as np\n",
    "NEW_FILE3 = 'D:/datasets/solar/new_data/new_data4.csv'\n",
    "data = np.loadtxt(NEW_FILE3,delimiter=',',skiprows=1,usecols=(0,1,3,4,5,6,7))#时间、月份、温度、湿度、辐射、漫反射、功率\n",
    "Ever_day = []\n",
    "for i in range(812):\n",
    "    tmp = data[i*21:i*21+21,:]\n",
    "    Ever_day.append(np.mean(tmp, 0))#按列求平均\n",
    "Ever_day = np.array(Ever_day)\n",
    "X = Ever_day[:,1:]\n",
    "from sklearn.cluster import DBSCAN,KMeans\n",
    "#db = KMeans(n_clusters=3).fit(X_scaler)\n",
    "#db = DBSCAN(eps = 0.4, min_samples = 10).fit(X_scaler)\n",
    "db = DBSCAN(eps = 28, min_samples = 5).fit(X)\n",
    "labels = db.labels_\n",
    "Ever_day = np.c_[Ever_day,labels]#在Every_day后增加一列\n",
    "class_one = []\n",
    "class_two = []\n",
    "class_three = []\n",
    "for row in Ever_day:\n",
    "    if row[-1] == -1:\n",
    "        class_one.append(row[0])\n",
    "    elif row[-1] == 0:\n",
    "        class_two.append(row[0])\n",
    "    else:\n",
    "        class_three.append(row[0])\n",
    "class_1 = class_two\n",
    "sub_class = class_one + class_three\n",
    "date,month,timepoint,temperature,humidity,radiation,diffuse,power = np.loadtxt(NEW_FILE3,delimiter=',',skiprows=1,unpack=True)\n",
    "from sklearn import preprocessing\n",
    "sub_date = []\n",
    "sub_temperature =[]\n",
    "sub_humidity = []\n",
    "sub_radiation = []\n",
    "sub_diffuse = []\n",
    "sub_power = []\n",
    "for i in range(len(date)):\n",
    "    if date[i] in sub_class:\n",
    "        sub_date.append(date[i])\n",
    "        sub_temperature.append(temperature[i])\n",
    "        sub_humidity.append(humidity[i])\n",
    "        sub_radiation.append(radiation[i])\n",
    "        sub_diffuse.append(diffuse[i])\n",
    "        sub_power.append(power[i])\n",
    "\n",
    "sub_date_d = []\n",
    "sub_temperature_d =[]\n",
    "sub_humidity_d = []\n",
    "sub_radiation_d = []\n",
    "sub_diffuse_d = []\n",
    "sub_power_d = []\n",
    "#按日的 温度 湿度  辐射 漫反射 功率\n",
    "for i in range(len(sub_class)):\n",
    "    sub_date_d.append(np.mean(sub_date[i*21:i*21+21]))\n",
    "    sub_temperature_d.append(\n",
    "        [np.max(sub_temperature[i*21:i*21+21]),np.mean(sub_temperature[i*21:i*21+21]),np.min(sub_temperature[i*21:i*21+21])])\n",
    "    sub_humidity_d.append(\n",
    "        [np.max(sub_humidity[i*21:i*21+21]),np.mean(sub_humidity[i*21:i*21+21]),np.min(sub_humidity[i*21:i*21+21])])\n",
    "    sub_radiation_d.append(np.mean(sub_radiation[i*21:i*21+21]))\n",
    "    sub_diffuse_d.append(np.mean(sub_diffuse[i*21:i*21+21]))\n",
    "    sub_power_d.append(np.mean(sub_power[i*21:i*21+21]))\n",
    "sub_date_d = np.reshape(sub_date_d,[-1,1])\n",
    "sub_radiation_d = np.reshape(sub_radiation_d,[-1,1])\n",
    "sub_diffuse_d = np.reshape(sub_diffuse_d,[-1,1])\n",
    "sub_power_d = np.reshape(sub_power_d,[-1,1])\n",
    "#归一化每日 温度 湿度 雨 辐射 漫反射\n",
    "day_temperature_scaler = preprocessing.MinMaxScaler().fit_transform(sub_temperature_d)\n",
    "day_humidity_scaler = preprocessing.MinMaxScaler().fit_transform(sub_humidity_d)\n",
    "day_radiation_scaler = preprocessing.MinMaxScaler().fit_transform(sub_radiation_d)\n",
    "day_diffuse_scaler = preprocessing.MinMaxScaler().fit_transform(sub_diffuse_d)\n",
    "day_power_scaler = preprocessing.MinMaxScaler().fit_transform(sub_power_d)\n",
    "\n",
    "sub_data = np.c_[sub_date_d,day_temperature_scaler,\n",
    "                 day_humidity_scaler,day_radiation_scaler,day_diffuse_scaler,day_power_scaler]\n",
    "X = sub_data[:,1:]\n",
    "db = DBSCAN(eps=0.32, min_samples=9).fit(X)\n",
    "labels = db.labels_\n",
    "p = set(labels)\n",
    "sub_data_labels = np.c_[sub_data,labels]\n",
    "class_2 = []\n",
    "class_3 = []\n",
    "for row in sub_data_labels:\n",
    "    if row[-1] == -1:\n",
    "        class_2.append(row[0])\n",
    "    else:\n",
    "        class_3.append(row[0])\n",
    "print(len(class_2),len(class_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 2 : 82.0\n",
      "测试数 16\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(NEW_FILE3,delimiter=',',skiprows=1)\n",
    "for i in range(len(data)):\n",
    "    if data[i][1] == 1:data[i][1] = 17.938\n",
    "    elif data[i][1] == 2:data[i][1] = 17.938\n",
    "    elif data[i][1] == 3:data[i][1] = 16.067\n",
    "    elif data[i][1] == 4:data[i][1] = 16.067\n",
    "    elif data[i][1] == 5:data[i][1] = 16.067\n",
    "    elif data[i][1] == 6:data[i][1] = 15.137\n",
    "    elif data[i][1] == 7:data[i][1] = 15.137\n",
    "    elif data[i][1] == 8:data[i][1] = 15.137\n",
    "    elif data[i][1] == 9:data[i][1] = 17.714\n",
    "    elif data[i][1] == 10:data[i][1] = 17.714\n",
    "    elif data[i][1] == 11:data[i][1] = 17.714\n",
    "    elif data[i][1] == 12:data[i][1] = 17.938\n",
    "    \n",
    "    if data[i][2] == 8.0:data[i][2] = 5.1096\n",
    "    elif data[i][2] == 8.5:data[i][2] = 8.9351\n",
    "    elif data[i][2] == 9.0:data[i][2] = 12.5525\n",
    "    elif data[i][2] == 9.5:data[i][2] = 15.4875\n",
    "    elif data[i][2] == 10.0:data[i][2] = 18.1367\n",
    "    elif data[i][2] == 10.5:data[i][2] = 20.2583\n",
    "    elif data[i][2] == 11.0:data[i][2] = 21.9231\n",
    "    elif data[i][2] == 11.5:data[i][2] = 23.1655\n",
    "    elif data[i][2] == 12.0:data[i][2] = 23.9481\n",
    "    elif data[i][2] == 12.5:data[i][2] = 24.0809\n",
    "    elif data[i][2] == 13.0:data[i][2] = 24.1543\n",
    "    elif data[i][2] == 13.5:data[i][2] = 23.6692\n",
    "    elif data[i][2] == 14.0:data[i][2] = 22.6423\n",
    "    elif data[i][2] == 14.5:data[i][2] = 21.1091\n",
    "    elif data[i][2] == 15.0:data[i][2] = 19.6702\n",
    "    elif data[i][2] == 15.5:data[i][2] = 17.5288\n",
    "    elif data[i][2] == 16.0:data[i][2] = 15.1380\n",
    "    elif data[i][2] == 16.5:data[i][2] = 12.1955\n",
    "    elif data[i][2] == 17.0:data[i][2] = 9.1260\n",
    "    elif data[i][2] == 17.5:data[i][2] = 5.4909\n",
    "    elif data[i][2] == 18.0:data[i][2] = 2.4679\n",
    "    \n",
    "    if data[i][-1] == 0:data[i][-1] = data[i][-3]*0.1\n",
    "\n",
    "skip_day = [1,16,31,38,69,81,126,182,197,280,301,320,330,413,484,498,579,622,636,651,683,739,749,860]\n",
    "#skip_day = skip_day + class_1 + class_2 + class_3 + class_4 + class_5 + class_6 + class_7\n",
    "skip_day = skip_day + class_1 + class_2\n",
    "all_data = []\n",
    "for i in range(17052):\n",
    "    if data[i][0] in skip_day:\n",
    "        continue\n",
    "    elif i%21 == 0:\n",
    "        all_data.append(np.array([data[i][0],data[i][1],data[i][2],data[i][3],data[i][4],data[i][5],\n",
    "                                  data[i][6],data[i-21][7],data[i-21][7],data[i][7]]))\n",
    "    else:\n",
    "        all_data.append(np.array([data[i][0],data[i][1],data[i][2],data[i][3],data[i][4],data[i][5],\n",
    "                                  data[i][6],data[i-21][7],data[i-1][7],data[i][7]]))\n",
    "print('class 2 :',len(all_data)/21)\n",
    "print('测试数',int(len(all_data)/105))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.33525801694644786\n",
      "  7.188247539384885\n",
      "  0.6703292237025212\n",
      "  0.37925560903386135\n",
      "  7.476856429040508\n",
      "3.209989363621645\n"
     ]
    }
   ],
   "source": [
    "f = []\n",
    "from sklearn import linear_model \n",
    "for i in [1,2,3,4,5]:\n",
    "    np.random.seed(i)\n",
    "    test_list = np.random.randint(0,int(len(all_data)/21),size = int(len(all_data)/105))#有重复\n",
    "    test_list = sorted(set(test_list))\n",
    "\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    train_date = []\n",
    "    test_date = []\n",
    "    for i in range(int(len(all_data)/21)):\n",
    "        for j in range(21):\n",
    "            a = all_data[i*21+j]\n",
    "            if i in test_list:\n",
    "                test_data.append(a[1:-1])\n",
    "                test_label.append(a[-1])\n",
    "                test_date.append(a[0])\n",
    "            else:\n",
    "                train_data.append(a[1:-1])\n",
    "                train_label.append(a[-1])\n",
    "                train_date.append(a[0])\n",
    "    LL = linear_model.LinearRegression()\n",
    "    LL.fit(train_data,train_label)\n",
    "    predict_power = LL.predict(test_data)\n",
    "    mape = [abs(predict_power[i]-test_label[i])/test_label[i] for i in range(len(test_label))]\n",
    "    f.append(np.mean(mape))\n",
    "    print(' ',np.mean(mape))\n",
    "print(np.mean(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LARs Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2100176886727936\n"
     ]
    }
   ],
   "source": [
    "f = []\n",
    "for i in [1,2,3,4,5]:\n",
    "    np.random.seed(i)\n",
    "    test_list = np.random.randint(0,int(len(all_data)/21),size = int(len(all_data)/105))#有重复\n",
    "    test_list = sorted(set(test_list))\n",
    "\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    train_date = []\n",
    "    test_date = []\n",
    "    for i in range(int(len(all_data)/21)):\n",
    "        for j in range(21):\n",
    "            a = all_data[i*21+j]\n",
    "            if i in test_list:\n",
    "                test_data.append(a[1:-1])\n",
    "                test_label.append(a[-1])\n",
    "                test_date.append(a[0])\n",
    "            else:\n",
    "                train_data.append(a[1:-1])\n",
    "                train_label.append(a[-1])\n",
    "                train_date.append(a[0])\n",
    "    \n",
    "    LL = linear_model.LassoLars(alpha = 0.000001)\n",
    "    LL.fit(train_data,train_label)\n",
    "    predict_power = LL.predict(test_data)\n",
    "    mape = [abs(predict_power[i]-test_label[i])/test_label[i] for i in range(len(test_label))]\n",
    "    f.append(np.mean(mape))\n",
    "print(np.mean(f))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.2701558988321223\n",
      "2 0.396580649107004\n",
      "3 0.45768745113219966\n",
      "4 0.33694070696315304\n",
      "5 0.32452789052660236\n",
      "0.3571785193122163\n"
     ]
    }
   ],
   "source": [
    "f = []\n",
    "from sklearn import svm\n",
    "for x in [1,2,3,4,5]:\n",
    "    np.random.seed(x)\n",
    "    test_list = np.random.randint(0,int(len(all_data)/21),size = int(len(all_data)/105))#有重复\n",
    "    test_list = sorted(set(test_list))\n",
    "\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    train_date = []\n",
    "    test_date = []\n",
    "    for i in range(int(len(all_data)/21)):\n",
    "        for j in range(21):\n",
    "            a = all_data[i*21+j]\n",
    "            if i in test_list:\n",
    "                test_data.append(a[1:-1])\n",
    "                test_label.append(a[-1])\n",
    "                test_date.append(a[0])\n",
    "            else:\n",
    "                train_data.append(a[1:-1])\n",
    "                train_label.append(a[-1])\n",
    "                train_date.append(a[0])\n",
    "\n",
    "    LL = svm.SVR(kernel = 'rbf', C = 1050, gamma = 0.0000215)\n",
    "    LL.fit(train_data,train_label)\n",
    "    predict_power = LL.predict(test_data)\n",
    "    mape = [abs(predict_power[i]-test_label[i])/test_label[i] for i in range(len(test_label))]\n",
    "    print(x,np.mean(mape))\n",
    "    f.append(np.mean(mape))\n",
    "print(np.mean(f))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epoch,loss value:27296.3\n",
      "After 50 epoch,loss value:2929.9\n",
      "After 100 epoch,loss value:2330.64\n",
      "After 150 epoch,loss value:2201.72\n",
      "After 200 epoch,loss value:2016.6\n",
      "Training Finished\n",
      "TEST MAPE: 0.3535278\n",
      "After 0 epoch,loss value:28204.8\n",
      "After 50 epoch,loss value:2212.64\n",
      "After 100 epoch,loss value:1808.49\n",
      "After 150 epoch,loss value:1700.16\n",
      "After 200 epoch,loss value:1646.41\n",
      "Training Finished\n",
      "TEST MAPE: 1.0325058\n",
      "After 0 epoch,loss value:29075\n",
      "After 50 epoch,loss value:2188.54\n",
      "After 100 epoch,loss value:1764.1\n",
      "After 150 epoch,loss value:1631.16\n",
      "After 200 epoch,loss value:1492.08\n",
      "Training Finished\n",
      "TEST MAPE: 0.5490363\n",
      "After 0 epoch,loss value:27425.4\n",
      "After 50 epoch,loss value:2142.55\n",
      "After 100 epoch,loss value:1918.07\n",
      "After 150 epoch,loss value:1849.26\n",
      "After 200 epoch,loss value:1826.35\n",
      "Training Finished\n",
      "TEST MAPE: 0.42921802\n",
      "After 0 epoch,loss value:27651.1\n",
      "After 50 epoch,loss value:2983.12\n",
      "After 100 epoch,loss value:1984.47\n",
      "After 150 epoch,loss value:1827.35\n",
      "After 200 epoch,loss value:1676.37\n",
      "Training Finished\n",
      "TEST MAPE: 0.69427407\n",
      "0.61171234\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "INPUT_NODE = 8\n",
    "OUTPUT_NODE = 1\n",
    "LAYER1_NODE = 160\n",
    "LAYER2_NODE = 140\n",
    "LAYER3_NODE = 120\n",
    "LAYER4_NODE = 100\n",
    "LAYER5_NODE = 90\n",
    "LAYER6_NODE = 80\n",
    "LAYER7_NODE = 70\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "REGULARIZATION_RATE = 0.0000001\n",
    "\n",
    "TRAINING_EPOCH = 201\n",
    "tf.set_random_seed(1)\n",
    "def inference(input_data, regularizer):\n",
    "    W1 = tf.Variable(tf.truncated_normal([INPUT_NODE,LAYER1_NODE],stddev=0.05))\n",
    "    b1 = tf.Variable(tf.constant(0.1,shape=[LAYER1_NODE]))\n",
    "    layer1 = tf.matmul(input_data,W1)+b1\n",
    "    layer1 = tf.maximum(layer1, 0.1*layer1)\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',regularizer(W1))\n",
    "    \n",
    "    W2 = tf.Variable(tf.truncated_normal([LAYER1_NODE,LAYER2_NODE],stddev=0.05))\n",
    "    b2 = tf.Variable(tf.constant(0.1,shape=[LAYER2_NODE]))\n",
    "    layer2 = tf.matmul(layer1,W2)+b2\n",
    "    layer2 = tf.maximum(layer2, 0.1*layer2)\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',regularizer(W2))\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([LAYER2_NODE,LAYER3_NODE],stddev=0.05))\n",
    "    b3 = tf.Variable(tf.constant(0.1,shape=[LAYER3_NODE]))\n",
    "    layer3 = tf.matmul(layer2,W3)+b3\n",
    "    layer3 = tf.maximum(layer3, 0.1*layer3)\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',regularizer(W3))\n",
    "    \n",
    "    W4 = tf.Variable(tf.truncated_normal([LAYER3_NODE,LAYER4_NODE],stddev=0.05))\n",
    "    b4 = tf.Variable(tf.constant(0.1,shape=[LAYER4_NODE]))\n",
    "    layer4 = tf.matmul(layer3,W4)+b4\n",
    "    layer4 = tf.maximum(layer4, 0.1*layer4)\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',regularizer(W4))\n",
    "        \n",
    "        \n",
    "    W5 = tf.Variable(tf.truncated_normal([LAYER4_NODE,LAYER5_NODE],stddev=0.05))\n",
    "    b5 = tf.Variable(tf.constant(0.1,shape=[LAYER5_NODE]))\n",
    "    layer5 = tf.matmul(layer4,W5)+b5\n",
    "    layer5 = tf.maximum(layer5, 0.1*layer5)\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',regularizer(W5))\n",
    "        \n",
    "        \n",
    "    W6 = tf.Variable(tf.truncated_normal([LAYER5_NODE,LAYER6_NODE],stddev=0.05))\n",
    "    b6 = tf.Variable(tf.constant(0.1,shape=[LAYER6_NODE]))\n",
    "    layer6 = tf.matmul(layer5,W6)+b6\n",
    "    layer6 = tf.maximum(layer6, 0.1*layer6)\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',regularizer(W6))\n",
    "        \n",
    "    W7 = tf.Variable(tf.truncated_normal([LAYER6_NODE,LAYER7_NODE],stddev=0.05))\n",
    "    b7 = tf.Variable(tf.constant(0.1,shape=[LAYER7_NODE]))\n",
    "    layer7 = tf.matmul(layer6,W7)+b7\n",
    "    layer7 = tf.maximum(layer7, 0.1*layer7)\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',regularizer(W6))\n",
    "    \n",
    "    W_o = tf.Variable(tf.truncated_normal([LAYER7_NODE,OUTPUT_NODE],stddev=0.05))\n",
    "    b_o = tf.Variable(tf.constant(0.1,shape=[OUTPUT_NODE]))\n",
    "    pre = tf.nn.relu(tf.matmul(layer7,W_o)+b_o)\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',regularizer(W_o))\n",
    "        \n",
    "    return pre\n",
    "\n",
    "input_data = tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "input_label = tf.placeholder(tf.float32,[None,OUTPUT_NODE])\n",
    "regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "\n",
    "pre = inference(input_data,regularizer)\n",
    "test_pre = inference(input_data,None)\n",
    "cost = tf.reduce_mean(tf.square(input_label - pre)) + tf.add_n(tf.get_collection('losses'))\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cost)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    f = []\n",
    "    for i in [1,2,3,4,5]:\n",
    "        np.random.seed(i)\n",
    "        test_list = np.random.randint(0,int(len(all_data)/21),size = 120)#有重复\n",
    "        test_list = sorted(set(test_list))\n",
    "\n",
    "        train_data = []\n",
    "        train_label = []\n",
    "        test_data = []\n",
    "        test_label = []\n",
    "        train_date = []\n",
    "        test_date = []\n",
    "        for i in range(int(len(all_data)/21)):\n",
    "            for j in range(21):\n",
    "                a = all_data[i*21+j]\n",
    "                if i in test_list:\n",
    "                    test_data.append(a[1:-1])\n",
    "                    test_label.append(a[-1])\n",
    "                    test_date.append(a[0])\n",
    "                else:\n",
    "                    train_data.append(a[1:-1])\n",
    "                    train_label.append(a[-1])\n",
    "                    train_date.append(a[0])\n",
    "        BATCH_SIZE = len(train_data)\n",
    "        tf.global_variables_initializer().run()\n",
    "        start = 0\n",
    "        end = start+BATCH_SIZE\n",
    "        for i in range(int(TRAINING_EPOCH*len(train_data)/BATCH_SIZE)):\n",
    "            xs = train_data[start:end]\n",
    "            ys = train_label[start:end]\n",
    "            try:\n",
    "                ys = np.reshape(ys,[BATCH_SIZE,1])\n",
    "            except:\n",
    "                print(len(ys))\n",
    "                print(len(train_data))\n",
    "            _, loss = sess.run([train_step,cost],feed_dict={input_data:xs, input_label:ys})\n",
    "            if end == len(train_data):\n",
    "                start = 0\n",
    "                end = BATCH_SIZE\n",
    "            else:\n",
    "                start = end\n",
    "                end+=BATCH_SIZE\n",
    "            if i%(50*len(train_data)/BATCH_SIZE) == 0:\n",
    "                print('After %d epoch,loss value:%g'%(i/(len(train_data)/BATCH_SIZE),loss))\n",
    "        print('Training Finished')\n",
    "\n",
    "        predict_power = sess.run(pre,feed_dict={input_data:test_data})\n",
    "\n",
    "        mape = [abs(predict_power[i]-test_label[i])/test_label[i] for i in range(21*len(test_list))]\n",
    "        print('TEST MAPE:',np.mean(mape))\n",
    "        f.append(np.mean(mape))\n",
    "    print(np.mean(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
